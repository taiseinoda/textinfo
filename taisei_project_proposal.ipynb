{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is the language in IPO prospectus informative?\n",
    "\n",
    "**Machine Learning/Dynamic Class (ECON 433/434)**\n",
    "\n",
    "\n",
    "**Project Proposal**\n",
    "\n",
    "\n",
    "**Taisei Noda**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "# Is the language in IPO prospectus informative?\n",
    "\n",
    "\n",
    "In this project, I will explore how informative the language in Initial Public Offering (IPO) S-1 filings is for predicting IPO performance. An S-1 filing is a registration statement submitted to the U.S. Securities and Exchange Commission (SEC) by a company seeking to go public. These filings are a crucial source of information for market participants, as newly public companies typically lack an established track record in public markets.\n",
    "\n",
    "S-1 filings contain both quantitative information—such as financial statements and offering terms (e.g., offering price and proceeds)—and qualitative disclosures, including textual descriptions of the company’s business model, market outlook, and strategic risks, as written from management’s perspective. This project investigates whether these textual components provide predictive information about IPO outcomes, beyond what is already captured by the standard financial metrics.\n",
    "\n",
    "More specifically, this project will address the following two questions:\n",
    "\n",
    "* Can the language in S-1 filings predict IPO initial returns, such as first-day or first-week performance?\n",
    "\n",
    "* Can text features from S-1s forecast whether a firm will be delisted or underperform over the next 3 to 5 years?\n",
    "\n",
    "The second question explores the long-term informativeness of S-1 disclosures, motivated by the observation that some IPO firms—particularly during hot market periods like the dot-com bubble—entered public markets with vague or overly optimistic business plans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Figures\n",
    "\n",
    "The figures to illustrate the main findings are as follows:\n",
    "\n",
    "• Model Performance Summary\n",
    "A scatter plot of actual vs. predicted IPO first-day (or first-week/month) returns.\n",
    "    – X-axis: Actual first-day return\n",
    "    – Y-axis: Predicted first-day return\n",
    "If the delisting classification model provides meaningful insights, a ROC curve would also be useful to illustrate classification accuracy compared to random guessing.\n",
    "\n",
    "• Bar Chart of Top Predictive Words\n",
    "This chart will display the top predictive phrases or keywords that the model associates with high or low IPO outcomes (e.g., initial returns or delisting risk).\n",
    "    – X-axis: Importance score (e.g., SHAP value, attention weight)\n",
    "    – Y-axis: Words (e.g., “uncertain,” “growth,” “profitable”)\n",
    "\n",
    "As for the model, I plan to start with FinBERT, which is specifically designed for financial text and seems well-suited for this task. However, I may also experiment with a simpler architecture such as a plain LSTM-based model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Data Collection\n",
    "\n",
    "* IPO Data\n",
    "  * [CRSP from wrds](https://wrds-www.wharton.upenn.edu/pages/about/data-vendors/center-for-research-in-security-prices-crsp/)\n",
    "  * [Jay Ritter's IPO Data](https://site.warrington.ufl.edu/ritter/ipo-data/)\n",
    "* S-1 Text\n",
    "  * [EDGAR](https://www.sec.gov/edgar/search/)\n",
    "\n",
    "I am familiar with the IPO data mentioned above, as I’ve worked with it in my own research project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S-1 Text Collection\n",
    "\n",
    "I have not collected all IPO firms yet, but I have built a code block to extract text from EDGAR and begun some basic word-level analysis, as shown below. It appears to be scalable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "from sec_edgar_downloader import Downloader\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "def setup(base_path=None,update=True):\n",
    "    if base_path is None:\n",
    "        if update == False:\n",
    "            if os.name == 'posix':\n",
    "            # MacOS (or Linux)\n",
    "                base_path = '/Users/taisei/Dropbox/'\n",
    "            elif os.name == 'nt':\n",
    "                # Windows\n",
    "                base_path = 'C:/Users/Taise/Dropbox/'\n",
    "        else:\n",
    "            base_path = input(\"Please enter your dropbox path: (e.g. C:/Users/Taise/Dropbox/)\")\n",
    "    data_path = os.path.join(base_path, \"IPOMatch/data/\")\n",
    "    figure_path = os.path.join(base_path, \"Apps/Overleaf/IPOMatch/tables_figures/\")\n",
    "    overleaf_path = os.path.join(base_path, \"Apps/Overleaf/IPOMatch/\")\n",
    "    print(f\"Data path: {data_path}\")\n",
    "    print(f\"Figure/Table path: {figure_path}\")\n",
    "    print(f\"Overleaf path: {overleaf_path}\")\n",
    "    return data_path,figure_path,overleaf_path\n",
    "\n",
    "def get_full_submission_path(data_path, cik, form_type='S-1'):\n",
    "    # Zero-pad the CIK\n",
    "    cik_padded = f\"{int(cik):010d}\"\n",
    "    filing_path = os.path.join(data_path, 'sec-edgar-filings', cik_padded, form_type)\n",
    "\n",
    "    # List subdirectories (accession number folders)\n",
    "    if not os.path.exists(filing_path):\n",
    "        raise FileNotFoundError(f\"No such directory: {filing_path}\")\n",
    "    \n",
    "    subdirs = [d for d in os.listdir(filing_path) if os.path.isdir(os.path.join(filing_path, d))]\n",
    "    if not subdirs:\n",
    "        raise FileNotFoundError(f\"No subfolders found in: {filing_path}\")\n",
    "    \n",
    "    # Sort by name or last modified time (optional)\n",
    "    subdirs.sort()  # or use sorted(subdirs, key=lambda x: ...)\n",
    "    target_folder = subdirs[0]  # or [-1] if you want the most recent\n",
    "    \n",
    "    full_path = os.path.join(filing_path, target_folder, 'full-submission.txt')\n",
    "    if not os.path.exists(full_path):\n",
    "        raise FileNotFoundError(f\"'full-submission.txt' not found in: {full_path}\")\n",
    "    \n",
    "    return full_path\n",
    "\n",
    "def extract_clean_text_from_html(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Remove script/style elements completely\n",
    "    for script_or_style in soup([\"script\", \"style\"]):\n",
    "        script_or_style.decompose()\n",
    "\n",
    "    text = soup.get_text(separator=' ')\n",
    "    text = re.sub(r'\\s+', ' ', text)  # collapse whitespace\n",
    "    return text.strip()\n",
    "\n",
    "def basic_token_cleanup(text, stopwords=None, top_n=50):\n",
    "    # Lowercase, remove punctuation/numbers\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # keep only letters and spaces\n",
    "\n",
    "    # Tokenize\n",
    "    words = text.split()\n",
    "\n",
    "    # Remove stopwords\n",
    "    if stopwords:\n",
    "        words = [w for w in words if w not in stopwords]\n",
    "\n",
    "    # Optional: remove short or \"junk\" tokens (like 'pt', 'td', 'tr')\n",
    "    words = [w for w in words if len(w) > 2 and not re.search(r'(font|style|display|align|div|inline|textindent|marginright|block|marginleft|width|shall|bold|roman|textdecoration)', w)]\n",
    "\n",
    "    return Counter(words).most_common(top_n)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path: C:/Users/Taise/Dropbox/IPOMatch/data/\n",
      "Figure/Table path: C:/Users/Taise/Dropbox/Apps/Overleaf/IPOMatch/tables_figures/\n",
      "Overleaf path: C:/Users/Taise/Dropbox/Apps/Overleaf/IPOMatch/\n"
     ]
    }
   ],
   "source": [
    "data_path,figure_path,overleaf_path = setup(update=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taise\\Directory\\ML2025Spring\\Projects\\Text\\ipo_functions.py:101: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sdc = pd.concat(dataframes, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated DataFrame is ready for analysis.\n",
      "issue_date_table.pkl and sdc_us_common.pkl are saved at C:/Users/Taise/Dropbox/IPOMatch/data/\n",
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "%run ipo_functions.py\n",
    "constructor = DataConstructor(data_path)\n",
    "sdc_us_common = constructor.import_sdc()\n",
    "all_matches,firm_lead_matches,uw_code_list = constructor.get_firm_uw_match(sdc_us_common)\n",
    "compustat_data = constructor.get_compustat_data(firm_lead_matches)\n",
    "text_data = constructor.get_10K_text_data(firm_lead_matches)\n",
    "crsp_data = constructor.get_crsp_data(firm_lead_matches)\n",
    "crsp_performance = constructor.compute_performance(crsp_data)\n",
    "ipo_firms1 = constructor.get_ipo_data()\n",
    "firms = constructor.merge_firm_data(compustat_data,text_data,crsp_performance,ipo_firms1)\n",
    "#uw_rank = constructor.construct_uw_rank(all_matches,uw_code_list)\n",
    "#uw_share = constructor.compute_uw_share(all_matches)\n",
    "#uw_industry_focus = constructor.describe_industry_focus_stats(all_matches)\n",
    "#uw_performance = constructor.underwriter_performance(crsp_performance,all_matches)\n",
    "#uw_data = constructor.merge_uw_data(uw_rank,uw_share,uw_performance,firms,firm_lead_matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#firms = pd.read_pickle(os.path.join(data_path, \"firms.pkl\"))\n",
    "ipo_firms = firms.dropna(subset=['IPOname','cik']).reset_index(drop=True)\n",
    "ipo_firms['cik'] = ipo_firms['cik'].astype(int)\n",
    "ipo_firms = ipo_firms[ipo_firms['ipo_issue_date'].dt.year >= 2001].reset_index(drop=True)\n",
    "ciks = ipo_firms.cik.dropna().unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_ciks = [1677077,1167178,864559,1169561,1401680]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/taisei/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from sec_edgar_downloader import Downloader\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize downloader\n",
    "company_name = \"Rice University\"\n",
    "email_address = \"tn27@rice.edu\"\n",
    "# Your sampled CIKs (as strings, padded to 10 digits)\n",
    "sampled_ciks_padded = [f\"{cik:010d}\" for cik in sampled_ciks]\n",
    "dl = Downloader(company_name=company_name, email_address=email_address,download_folder=data_path)\n",
    "# Loop and fetch S-1 filings\n",
    "for cik in sampled_ciks_padded:\n",
    "    dl.get(\"S-1\", cik)  # You can increase 'amount' for more filings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('stock', 913), ('may', 754), ('shares', 708), ('company', 612), ('common', 606), ('securities', 351), ('agreement', 330), ('date', 317), ('financial', 310), ('price', 301), ('board', 290), ('offering', 289), ('business', 267), ('corporation', 267), ('per', 266), ('upon', 255), ('clinical', 254), ('share', 253), ('directors', 248), ('product', 245), ('purchase', 240), ('note', 236), ('number', 208), ('additional', 207), ('time', 200), ('april', 200), ('officer', 189), ('could', 189), ('development', 189), ('fda', 189), ('act', 187), ('related', 183), ('stockholders', 182), ('approval', 180), ('including', 179), ('meeting', 178), ('required', 177), ('market', 175), ('future', 169), ('alzheimers', 169), ('executive', 168), ('value', 168), ('license', 166), ('section', 165), ('subject', 161), ('operations', 161), ('regulatory', 160), ('months', 160), ('interest', 158), ('exercise', 157), ('years', 154), ('pursuant', 153), ('compensation', 152), ('director', 151), ('issued', 150), ('notice', 149), ('public', 148), ('capital', 147), ('products', 145), ('ended', 145), ('outstanding', 145), ('net', 143), ('options', 142), ('statements', 141), ('based', 139), ('information', 138), ('prospectus', 138), ('ault', 138), ('new', 136), ('certain', 136), ('party', 136), ('effect', 136), ('results', 135), ('would', 135), ('person', 135), ('year', 130), ('fees', 129), ('january', 128), ('period', 127), ('expenses', 127), ('convertible', 127), ('cash', 127), ('terms', 126), ('option', 124), ('warrants', 124), ('rights', 122), ('total', 121), ('issuance', 121), ('companys', 121), ('plan', 120), ('effective', 117), ('action', 117), ('term', 117), ('paid', 117), ('also', 115), ('applicable', 114), ('otherwise', 112), ('lithium', 112), ('provided', 111), ('amount', 108)]\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "cik = sampled_ciks_padded[0]  # Or any padded CIK like '0001401680'\n",
    "file_path = get_full_submission_path(data_path, cik)\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "clean_text = extract_clean_text_from_html(content)\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "top_words = basic_token_cleanup(clean_text, stopwords=stop_words, top_n=100)\n",
    "print(top_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
